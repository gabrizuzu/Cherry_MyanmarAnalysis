{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import paramiko\n",
    "\n",
    "# Configurazioni\n",
    "BASE_URL = \"http://data.gdeltproject.org/gdeltv2\"\n",
    "START_DATE = datetime(2025, 1, 1)       # Data di inizio (1 gennaio 2020)\n",
    "END_DATE = datetime.now()               # Data di fine (oggi)\n",
    "\n",
    "# Configurazioni SFTP\n",
    "SFTP_HOST = \"93.43.225.19\"\n",
    "SFTP_PORT = 20022\n",
    "SFTP_USERNAME = \"cherryuser\"            # Sostituisci con il tuo username\n",
    "SFTP_PASSWORD = \"\"          # Sostituisci con la tua password\n",
    "SFTP_REMOTE_DIR = \"/data/Gabriele\"      # Cartella remota sul server\n",
    "\n",
    "# Funzione per scaricare un file e caricarlo via SFTP\n",
    "def download_and_upload_file(date):\n",
    "    filename = f\"{date.strftime('%Y%m%d%H%M%S')}.gkg.csv.zip\"\n",
    "    url = f\"{BASE_URL}/{filename}\"\n",
    "\n",
    "    print(f\"Scaricando {url}...\")\n",
    "    try:\n",
    "        # Scarica il file\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            # Salva il file temporaneamente\n",
    "            temp_path = f\"/tmp/{filename}\"\n",
    "            with open(temp_path, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(f\"File scaricato: {temp_path}\")\n",
    "\n",
    "            # Carica il file sul server via SFTP\n",
    "            upload_via_sftp(temp_path, filename)\n",
    "            print(f\"File caricato sul server: {filename}\")\n",
    "\n",
    "            # Elimina il file temporaneo\n",
    "            os.remove(temp_path)\n",
    "        else:\n",
    "            print(f\"File non trovato: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il download: {e}\")\n",
    "\n",
    "# Funzione per caricare un file via SFTP\n",
    "def upload_via_sftp(local_path, remote_filename):\n",
    "    try:\n",
    "        # Connessione al server SFTP\n",
    "        transport = paramiko.Transport((SFTP_HOST, SFTP_PORT))\n",
    "        transport.connect(username=SFTP_USERNAME, password=SFTP_PASSWORD)\n",
    "        sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "        # Carica il file\n",
    "        remote_path = f\"{SFTP_REMOTE_DIR}/{remote_filename}\"\n",
    "        sftp.put(local_path, remote_path)\n",
    "        sftp.close()\n",
    "        transport.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il caricamento via SFTP: {e}\")\n",
    "\n",
    "# Loop attraverso tutte le date dal 2020 a oggi\n",
    "current_date = START_DATE\n",
    "while current_date <= END_DATE:\n",
    "    download_and_upload_file(current_date)\n",
    "    current_date += timedelta(minutes=15)  # ogni 15\n",
    "\n",
    "print(\"Download e caricamento completati!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prova con il server dell'ufficio, ha dato problemi ad accedere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 6875 file da processare.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download in corso: 100%|██████████| 6875/6875 [31:32<00:00,  3.63file/s]\n",
      "Processamento in corso: 100%|██████████| 6875/6875 [14:30<00:00,  7.90file/s]   \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm  # Libreria per la barra di avanzamento\n",
    "\n",
    "# Configurazioni\n",
    "DEST_DIR = \"/Volumes/CHERRY_SSD/gdelt_gkg_2025\"\n",
    "MASTERFILE_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
    "START_DATE = \"20240101\"  # Data di inizio\n",
    "END_DATE= \"20241231\"\n",
    "MYANMAR_KEYWORD = \"Myanmar\"\n",
    "\n",
    "# Lista delle colonne GKG\n",
    "GKG_COLUMNS = [\n",
    "    \"GKGRECORDID\", \"DATE\", \"SourceCollectionIdentifier\", \"SourceCommonName\", \"DocumentIdentifier\",\n",
    "    \"Counts\", \"V2Counts\", \"Themes\", \"V2Themes\", \"Locations\", \"V2Locations\",\n",
    "    \"Persons\", \"V2Persons\", \"Organizations\", \"V2Organizations\", \"V2Tone\",\n",
    "    \"Dates\", \"GCAM\", \"SharingImage\", \"RelatedImages\", \"SocialImageEmbeds\",\n",
    "    \"SocialVideoEmbeds\", \"Quotations\", \"AllNames\", \"Amounts\", \"TranslationInfo\", \"ExtrasXML\"\n",
    "]\n",
    "\n",
    "\n",
    "# Crea la cartella se non esiste\n",
    "Path(DEST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_masterfile_list():\n",
    "    \"\"\"Scarica la lista dei file disponibili\"\"\"\n",
    "    response = requests.get(MASTERFILE_URL)\n",
    "    if response.status_code == 200:\n",
    "        return response.text.split(\"\\n\")\n",
    "    else:\n",
    "        print(\"Errore nel download della masterfilelist.\")\n",
    "        return []\n",
    "\n",
    "def filter_gkg_files(file_list):\n",
    "    \"\"\"Filtra i file GKG dal 1 gennaio 2025\"\"\"\n",
    "    gkg_files = []\n",
    "    for line in file_list:\n",
    "        parts = line.split()\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        url = parts[2]\n",
    "        if \"gkg\" in url and url.endswith(\".zip\"):\n",
    "            filename = os.path.basename(url)\n",
    "            date_str = filename.split(\".\")[0][:8]  # Prendi solo AAAAMMGG\n",
    "            if date_str >= START_DATE:\n",
    "                gkg_files.append(url)\n",
    "    return gkg_files\n",
    "\n",
    "def download_file(url, dest_dir):\n",
    "    \"\"\"Scarica un file da un URL e lo salva nella directory di destinazione\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        file_path = os.path.join(dest_dir, os.path.basename(url))\n",
    "        with open(file_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        return file_path\n",
    "    return None\n",
    "\n",
    "def process_gkg_file(zip_path, output_dir):\n",
    "    \"\"\"Estrae, filtra Myanmar e salva il risultato\"\"\"\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            for file in z.namelist():\n",
    "                with z.open(file) as f:\n",
    "                    try:\n",
    "                        df = pd.read_csv(f, delimiter=\"\\t\", dtype=str, names=GKG_COLUMNS, encoding='utf-8')\n",
    "                    except UnicodeDecodeError:\n",
    "                        try:\n",
    "                            df = pd.read_csv(f, delimiter=\"\\t\", dtype=str, names=GKG_COLUMNS, encoding='ISO-8859-1')\n",
    "                        except UnicodeDecodeError:\n",
    "                            return  # Se anche latin-1 fallisce, saltiamo il file\n",
    "\n",
    "                    df = df[df[\"V2Locations\"].str.contains(MYANMAR_KEYWORD, na=False)]\n",
    "                    if not df.empty:\n",
    "                        output_file = os.path.join(output_dir, \"gkg_myanmar_filtered.csv\")\n",
    "                        df.to_csv(output_file, mode=\"a\", header=not os.path.exists(output_file), index=False)\n",
    "\n",
    "        os.remove(zip_path)  # Elimina il file ZIP dopo il processamento\n",
    "    except Exception:\n",
    "        os.remove(zip_path)  # Elimina il file ZIP anche in caso di errore\n",
    "\n",
    "def download_and_process_parallel():\n",
    "    \"\"\"Pipeline parallela: scarica e processa i file con barra di avanzamento\"\"\"\n",
    "    masterfile_list = get_masterfile_list()\n",
    "    gkg_files = filter_gkg_files(masterfile_list)\n",
    "\n",
    "    print(f\"Trovati {len(gkg_files)} file da processare.\")\n",
    "\n",
    "    # Barra di avanzamento per il download\n",
    "    with tqdm(total=len(gkg_files), desc=\"Download in corso\", unit=\"file\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = {executor.submit(download_file, url, DEST_DIR): url for url in gkg_files}\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                zip_path = future.result()\n",
    "                if zip_path:\n",
    "                    pbar.update(1)\n",
    "\n",
    "    # Elenco file scaricati\n",
    "    downloaded_files = [os.path.join(DEST_DIR, os.path.basename(url)) for url in gkg_files if os.path.exists(os.path.join(DEST_DIR, os.path.basename(url)))]\n",
    "\n",
    "    # Barra di avanzamento per il processamento\n",
    "    with tqdm(total=len(downloaded_files), desc=\"Processamento in corso\", unit=\"file\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = {executor.submit(process_gkg_file, zip_path, DEST_DIR): zip_path for zip_path in downloaded_files}\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_and_process_parallel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questo li scarica tutti e poi li processa --> problema di memoria\n",
    "velocità: 2 mesi e mezzo di dati in 45 minuti circa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 35134 file da processare.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download e processamento:   0%|          | 31/35134 [00:10<2:59:11,  3.26file/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm  # Barra di avanzamento\n",
    "\n",
    "# Configurazioni\n",
    "DEST_DIR = \"/Volumes/CHERRY_SSD/gdelt_gkg_2023\"\n",
    "MASTERFILE_URL = \"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\"\n",
    "START_DATE = \"20230101\"  # Data di inizio\n",
    "END_DATE= \"20231231\"\n",
    "MYANMAR_KEYWORD = \"Myanmar\"\n",
    "\n",
    "# Lista delle colonne GKG\n",
    "GKG_COLUMNS = [\n",
    "    \"GKGRECORDID\", \"DATE\", \"SourceCollectionIdentifier\", \"SourceCommonName\", \"DocumentIdentifier\",\n",
    "    \"Counts\", \"V2Counts\", \"Themes\", \"V2Themes\", \"Locations\", \"V2Locations\",\n",
    "    \"Persons\", \"V2Persons\", \"Organizations\", \"V2Organizations\", \"V2Tone\",\n",
    "    \"Dates\", \"GCAM\", \"SharingImage\", \"RelatedImages\", \"SocialImageEmbeds\",\n",
    "    \"SocialVideoEmbeds\", \"Quotations\", \"AllNames\", \"Amounts\", \"TranslationInfo\", \"ExtrasXML\"\n",
    "]\n",
    "\n",
    "# Crea la cartella se non esiste\n",
    "Path(DEST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_masterfile_list():\n",
    "    \"\"\"Scarica la lista dei file disponibili\"\"\"\n",
    "    response = requests.get(MASTERFILE_URL)\n",
    "    if response.status_code == 200:\n",
    "        return response.text.split(\"\\n\")\n",
    "    else:\n",
    "        print(\"Errore nel download della masterfilelist.\")\n",
    "        return []\n",
    "\n",
    "def filter_gkg_files(file_list):\n",
    "    \"\"\"Filtra i file GKG dal 1 gennaio 2025\"\"\"\n",
    "    gkg_files = []\n",
    "    for line in file_list:\n",
    "        parts = line.split()\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        url = parts[2]\n",
    "        if \"gkg\" in url and url.endswith(\".zip\"):\n",
    "            filename = os.path.basename(url)\n",
    "            date_str = filename.split(\".\")[0][:8]  # Prendi solo AAAAMMGG\n",
    "            if date_str >= START_DATE and date_str <= END_DATE:\n",
    "                gkg_files.append(url)\n",
    "    return gkg_files\n",
    "\n",
    "def download_and_process_file(url):\n",
    "    \"\"\"Scarica un file ZIP, lo processa e poi lo elimina\"\"\"\n",
    "    try:\n",
    "        # Scarica il file ZIP\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            return False  # Errore nel download\n",
    "        \n",
    "        zip_path = os.path.join(DEST_DIR, os.path.basename(url))\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "        # Processa il file\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            for file in z.namelist():\n",
    "                with z.open(file) as f:\n",
    "                    try:\n",
    "                        df = pd.read_csv(f, delimiter=\"\\t\", dtype=str, names=GKG_COLUMNS, encoding='utf-8')\n",
    "                    except UnicodeDecodeError:\n",
    "                        try:\n",
    "                            df = pd.read_csv(f, delimiter=\"\\t\", dtype=str, names=GKG_COLUMNS, encoding='ISO-8859-1')\n",
    "                        except UnicodeDecodeError:\n",
    "                            os.remove(zip_path)  # Se non può essere letto, eliminiamo il file\n",
    "                            return False  \n",
    "\n",
    "                    # Filtra solo i record che contengono Myanmar\n",
    "                    df = df[df[\"V2Locations\"].str.contains(MYANMAR_KEYWORD, na=False)]\n",
    "\n",
    "                    # Salva nel file aggregato\n",
    "                    if not df.empty:\n",
    "                        output_file = os.path.join(DEST_DIR, \"gkg_myanmar_filtered.csv\")\n",
    "                        df.to_csv(output_file, mode=\"a\", header=not os.path.exists(output_file), index=False)\n",
    "\n",
    "        os.remove(zip_path)  # Elimina il file ZIP dopo il processamento\n",
    "        return True  # Successo\n",
    "\n",
    "    except Exception:\n",
    "        if os.path.exists(zip_path):\n",
    "            os.remove(zip_path)  # Elimina il file anche in caso di errore\n",
    "        return False\n",
    "\n",
    "def download_and_process_parallel():\n",
    "    \"\"\"Pipeline parallela: scarica e processa i file uno alla volta\"\"\"\n",
    "    masterfile_list = get_masterfile_list()\n",
    "    gkg_files = filter_gkg_files(masterfile_list)\n",
    "\n",
    "    print(f\"Trovati {len(gkg_files)} file da processare.\")\n",
    "\n",
    "    with tqdm(total=len(gkg_files), desc=\"Download e processamento\", unit=\"file\") as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = {executor.submit(download_and_process_file, url): url for url in gkg_files}\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)  # Aggiorna la barra di avanzamento\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_and_process_parallel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questo li scarica e processa subito e poi elimina e continua--> forse troppo lento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
